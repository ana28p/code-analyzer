{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from pyclustertend import hopkins, vat, assess_tendency_by_mean_metric_score\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, minmax_scale, RobustScaler,robust_scale, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file = \"/metrics.csv\"\n",
    "    data = pd.read_csv(file, sep=';')\n",
    "    return data\n",
    "\n",
    "merged_data = get_data()\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = merged_data.columns.tolist()\n",
    "list_columns.remove('Method')\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = merged_data.copy()\n",
    "\n",
    "for col_name in list_columns:\n",
    "    col = scaled_data[col_name]\n",
    "    min_col, max_col = col.min(), col.max()\n",
    "    min_col = 0  # consider min as 0 to perserve the importance of values; eg LOC 25, 50 -> 0.5, 1 \n",
    "#     print(col_name, min_col, max_col)\n",
    "    new_min, new_max = 0, 100\n",
    "    scaled_data[col_name] = new_min + ((col - min_col) * (new_max - new_min) / (max_col - min_col))\n",
    "    \n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data[list_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data[list_columns]\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = cluster.DBSCAN(eps=20)\n",
    "dbscan.fit(scaled_data[list_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = scaled_data[scaled_data['CLevel'] == -1]\n",
    "first = scaled_data[scaled_data['CLevel'] == 0]\n",
    "# second = scaled_data[scaled_data['CLevel'] == 1] \n",
    "# thrid = scaled_data[scaled_data['CLevel'] == 2]\n",
    "\n",
    "# print(len(first), len(second), len(thrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string_label(value):\n",
    "    if value == 0:\n",
    "        return \"low\"\n",
    "    elif value == 1:\n",
    "        return \"regular\"\n",
    "    elif value == 2:\n",
    "        return \"high\"\n",
    "    elif value == -1:\n",
    "        return \"noise\"\n",
    "scaled_data['CLevelLbl'] = scaled_data['CLevel'].apply(to_string_label)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,10))\n",
    "ax = axes.flatten()\n",
    "for i in range(len(list_columns)):\n",
    "    col_name = list_columns[i]\n",
    "    sns.stripplot(data=scaled_data, x='CLevelLbl', y=col_name, \n",
    "#                   hue='CLevelLbl', \n",
    "                  palette={'low':'blue', 'regular':'#DCB732', 'high':'red', 'noise': 'black'},\n",
    "                  jitter=0.25, size=8, ax=ax[i], linewidth=.5,\n",
    "#                   order=[\"low\", \"regular\", \"high\"]\n",
    "                 )\n",
    "    ax[i].set_ylabel(col_name)\n",
    "    ax[i].set_xlabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scaled_data.copy()\n",
    "list_columns = ['LOC', 'CC', 'NP', 'NV', 'NEST', 'Ca', 'Ce', 'NChg', 'NCall']\n",
    "# df = df.stack()\n",
    "df = pd.melt(df, id_vars=['Method', 'CLevelLbl'], value_vars=list_columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10), dpi= 80)    \n",
    "sns.stripplot(data=df, x='variable', y='value', hue='CLevelLbl',\n",
    "              palette={'low':'blue', 'regular':'#DCB732', 'high':'red'},\n",
    "              hue_order=[\"low\", \"regular\", \"high\"],\n",
    "              jitter=0.25, size=8, ax=ax, linewidth=.5, dodge=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['CLevel'] = groups\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string_label(value):\n",
    "    if value == 0:\n",
    "        return \"low\"\n",
    "    elif value == 1:\n",
    "        return \"regular\"\n",
    "    if value == 2:\n",
    "        return \"high\"\n",
    "merged_data['CLevel'] = merged_data['CLevel'].apply(to_string_label)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_critical = len(merged_data[merged_data['CLevel'] == \"low\"])\n",
    "regular_critical = len(merged_data[merged_data['CLevel'] == \"regular\"]) \n",
    "high_critical = len(merged_data[merged_data['CLevel'] == \"high\"])\n",
    "\n",
    "print(low_critical, regular_critical, high_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_critical_data = merged_data[merged_data['CLevel'] == \"high\"]\n",
    "high_critical_data.to_csv(\"C:/Users/aprodea/work/metrics-tax-compare/merged/high_critical_data_em_gmm.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywaffle import Waffle\n",
    "\n",
    "data = {'Low critical': low_critical, 'Regular critical': regular_critical, 'High critical': high_critical}\n",
    "\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=10, \n",
    "    values=data, \n",
    "    colors=(\"#232066\", \"#DCB732\", \"#983D3D\"),\n",
    "    labels=['Low critical', 'Regular critical', 'High critical'],\n",
    "    legend={'loc': 'lower left', 'bbox_to_anchor': (0, -0.4), 'ncol': len(data), 'framealpha': 0},\n",
    "    interval_ratio_x=0.5,\n",
    "    interval_ratio_y=0.5,\n",
    "    figsize=(20,10)\n",
    ")\n",
    "fig.gca().set_facecolor('#EEEEEE')\n",
    "fig.set_facecolor('#EEEEEE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = merged_data.copy()\n",
    "\n",
    "def split_at_last_point(s):\n",
    "    idx = s.rfind('.')\n",
    "    return s[:idx], s[idx+1:]\n",
    "def split_method_name(value):\n",
    "    parent, method_name = split_at_last_point(value)\n",
    "    parent, class_name = split_at_last_point(parent)\n",
    "    return pd.Series([parent, class_name, method_name])\n",
    "\n",
    "data_class[['Parent_class', 'Class', 'Method']] = data_class['Method'].apply(split_method_name)\n",
    "data_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
