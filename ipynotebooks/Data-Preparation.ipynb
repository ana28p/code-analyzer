{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepartion\n",
    "These scripts load the reports from source code metrics (NDepend), repository mining (PyDriller) and profiler (dotTrace) and filter and change the methods name in order to match the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.ticker import StrMethodFormatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code_metrics_file = \"/ndepend/export_query_v12.csv\"\n",
    "repo_mining_file = \"/commits-to-v12.0.0.csv\"\n",
    "usage_folder = \"/profiler/usage/\"\n",
    "\n",
    "repo_mining_file_for_chglines = None\n",
    "\n",
    "test_coverage_file = None\n",
    "\n",
    "save_to_folder = \"/merged/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper methods to replace parameter types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {'byte': 'Byte', 'sbyte': 'SByte', 'int': 'Int32', 'uint': 'UInt32', 'short': 'Int16', 'ushort': 'UInt16',\n",
    "             'long': 'Int64', 'ulong': 'UInt64', 'float': 'Single', 'double': 'Double', 'char': 'Char',\n",
    "             'bool': 'Boolean', 'object': 'Object', 'string': 'String', 'decimal': 'Decimal', 'dynamic': 'Object'}\n",
    "\n",
    "def get_type_outbox(param_type):\n",
    "    if param_type in type_dict:\n",
    "        return type_dict[param_type]\n",
    "    return param_type\n",
    "\n",
    "\n",
    "def handle_token(token, remove_parentclass, replace_type):\n",
    "    if remove_parentclass:\n",
    "        token = token[token.rfind(\".\") + 1:]\n",
    "    if replace_type:\n",
    "        token = get_type_outbox(token)\n",
    "    return token\n",
    "\n",
    "\n",
    "def replace_by_token(param_type, remove_parentclass, replace_type):\n",
    "    delims = \"<>,[]()\"\n",
    "    token = ''\n",
    "    new_param_type = ''\n",
    "    for i in range(len(param_type)):\n",
    "        if (param_type[i] in delims):\n",
    "            replace = handle_token(token, remove_parentclass, replace_type)\n",
    "            token = ''\n",
    "            new_param_type += replace + param_type[i]\n",
    "        else:\n",
    "            token = token + param_type[i]\n",
    "            if (i == len(param_type) - 1):\n",
    "                replace = handle_token(token, remove_parentclass, replace_type)\n",
    "                new_param_type += replace\n",
    "    return new_param_type\n",
    "\n",
    "\n",
    "def replace_types(param_type):\n",
    "    for k,v in type_dict.items():\n",
    "        if k in param_type:\n",
    "            param_type = param_type.replace(k, v)\n",
    "    return param_type\n",
    "\n",
    "\n",
    "def ctor_to_class_name(value):\n",
    "    if \"..ctor\" in value:\n",
    "        idx2 = value.rfind(\"..ctor\")\n",
    "        idx1 = value[:idx2].rfind(\".\")\n",
    "        cls_name = value[idx1:idx2]\n",
    "        # in case the class has generics keep only the class name\n",
    "        if '<' in cls_name:\n",
    "            cls_name = cls_name[:cls_name.find('<')]\n",
    "        value = value.replace(\"..ctor\", cls_name)\n",
    "    if \"..cctor\" in value:\n",
    "        idx2 = value.rfind(\"..cctor\")\n",
    "        idx1 = value[:idx2].rfind(\".\")\n",
    "        cls_name = value[idx1:idx2]\n",
    "        # in case the class has generics keep only the class name\n",
    "        if '<' in cls_name:\n",
    "            cls_name = cls_name[:cls_name.find('<')]\n",
    "        value = value.replace(\"..cctor\", cls_name)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source code metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_method_name_metrics(method):\n",
    "    # for inner classes\n",
    "    method = method.replace('+', '.')\n",
    "    # for out, ref paramaters\n",
    "    method = method.replace('&', '')\n",
    "    # some parameter types include also the parent; and other reports don't include it eg SqlMapper.GridReader\n",
    "    start_parameters = method.rfind('(')\n",
    "    method_name = method[:start_parameters]\n",
    "    parameters_text = method[start_parameters:]\n",
    "    if len(parameters_text) > 2:\n",
    "        parameters_text = replace_by_token(parameters_text, True, False)\n",
    "\n",
    "    new_name = method_name + parameters_text\n",
    "            \n",
    "    return new_name\n",
    "\n",
    "\n",
    "def get_code_metrics_data(file):\n",
    "    data = pd.read_csv(file, sep=';', decimal=',')\n",
    "    data['FullName'] = data['FullName'].apply(change_method_name_metrics)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = get_code_metrics_data(source_code_metrics_file)\n",
    "metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data[metrics_data['FullName'].str.contains(\".Migrations.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data by excluding setter, getters, anonymous types and methods with LOC not set or 0.\n",
    "The final dataset contains the set of metrics used in the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics_data[\"NbLinesOfCode\"].replace({0: np.nan}, inplace=True)\n",
    "\n",
    "no_getters = metrics_data['IsPropertyGetter'] == False\n",
    "no_setters = metrics_data['IsPropertySetter'] == False\n",
    "no_operators = metrics_data['IsOperator'] == False\n",
    "no_empty_method = metrics_data['NbLinesOfCode'].notna()\n",
    "annonymous = metrics_data['FullName'].str.contains(\"f__AnonymousType\")\n",
    "migrations = metrics_data['FullName'].str.contains(\".Migrations.\")\n",
    "\n",
    "filtered_metrics_data = metrics_data.copy()\n",
    "filtered_metrics_data = filtered_metrics_data[no_setters & no_getters & no_operators & no_empty_method & ~annonymous & ~migrations].reset_index()\n",
    "\n",
    "sc_metrics_data = filtered_metrics_data[[\"FullName\", \"NbLinesOfCode\", \"CyclomaticComplexity\", \"NbParameters\", \"NbVariables\",\n",
    "                                \"ILNestingDepth\", \"NbMethodsCallingMe\", \"NbMethodsCalledInternal\"]].copy()\n",
    "sc_metrics_data.columns = [\"Method\", \"LOC\", \"CC\", \"NP\", \"NV\", \"NEST\", \"Ca\", \"Ce\"]\n",
    "\n",
    "# Constructors are listed as 'ctor' or 'cctor', replace these with the class name (the actual name of the constructor)\n",
    "sc_metrics_data['Method'] = sc_metrics_data['Method'].apply(ctor_to_class_name)\n",
    "# ..ctor and ..cctor might result in the same method; however the source doesn't have all the constructors find by the Ndepend\n",
    "sc_metrics_data.drop_duplicates(subset=['Method'], inplace=True, ignore_index=True)\n",
    "sc_metrics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository mining metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_method_name_commits(method):\n",
    "    if method is np.nan:\n",
    "        return method\n",
    "    \n",
    "    method = method.replace('::', '.')\n",
    "    start_parameters = method.rfind('(')\n",
    "    method_name = method[:start_parameters]\n",
    "    parameters_text = method[start_parameters:]\n",
    "    if len(parameters_text) > 2:\n",
    "        parameters_text = parameters_text[1:-1]  # remove ()\n",
    "        # remove [FromBody] from parameters text\n",
    "        parameters_text = parameters_text.replace(\"[ FromBody ] \", \"\")\n",
    "        parameters = parameters_text.split(', ')\n",
    "        params_types = []\n",
    "        for param in parameters:\n",
    "            if \"=\" in param:\n",
    "                param = param[:param.find(\"=\")]\n",
    "                \n",
    "            # remove potential spaces from start and end\n",
    "            param = param.strip(\" \")\n",
    "            \n",
    "            p = param.split(' ')\n",
    "            if \" ? \" in param:\n",
    "                # if int ? _ -> Nullable<Int32>\n",
    "                p_t = replace_by_token(p[0], True, True)\n",
    "                param_type = \"Nullable<\" + p_t + \">\"\n",
    "                if \"[]\" in ''.join(p[0:-1]):\n",
    "                    param_type = param_type + \"[]\"\n",
    "            else:\n",
    "                start = 0\n",
    "                end = -1\n",
    "                if (p[0] == \"params\") or (p[0] == \"this\") or (p[0] == \"out\") or (p[0] == \"in\") or (p[0] == \"ref\"):\n",
    "                    # if params string [] _ -> String[] or params Func<T,object> [] _ -> Func<T,Object>[]\n",
    "                    start = 1\n",
    "                p_t = ''.join(p[start:end])\n",
    "                param_type = replace_by_token(p_t, True, True)\n",
    "\n",
    "            params_types.append(param_type)\n",
    "        parameters_text = '(' + ','.join(params_types) + ')'\n",
    "\n",
    "    new_name = method_name + parameters_text\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def get_change_metrics_data(file):\n",
    "    data = pd.read_csv(file, sep=';')\n",
    "    data['Method_Parsed'] = data['Method'].apply(change_method_name_commits)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load change metrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_data = get_change_metrics_data(repo_mining_file)\n",
    "change_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for number of changed lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lines_data = None\n",
    "change_lines_data_org = None\n",
    "if repo_mining_file_for_chglines is not None:\n",
    "    change_lines_data = get_change_metrics_data(repo_mining_file_for_chglines)\n",
    "    change_lines_data['Previous_Method_Parsed'] = change_lines_data['Previous_name'].apply(change_method_name_commits)\n",
    "    change_lines_data_org = change_lines_data\n",
    "    change_lines_data = change_lines_data[['Method_Parsed', 'Previous_Method_Parsed', 'ChgLines']]\n",
    "    change_lines_data.to_csv(save_to_folder + \"change_lines.csv\", sep=';', index=False)\n",
    "change_lines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if change_lines_data_org is not None:\n",
    "    print(change_lines_data_org['Previous_name'].isna().sum())\n",
    "    diff_chg = change_data[~change_data['Method'].isin(change_lines_data_org['Previous_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_data[~change_data['Method_Parsed'].isin(change_lines_data['Previous_Method_Parsed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc_metrics_data[sc_metrics_data['Method'].isin(change_lines_data['Previous_Method_Parsed'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if change_lines_data is not None:\n",
    "    df = pd.merge(change_data, change_lines_data[['Previous_Method_Parsed', 'ChgLines']], how='inner', left_on='Method', right_on='Previous_Method_Parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def change_method_name_usage(method):\n",
    "    if '`' in method:\n",
    "        idx = method.find('`')\n",
    "        part = method[idx+1:]\n",
    "        method = method[:idx] + part[part.find('.'):]\n",
    "    params_start = method.rfind('(')\n",
    "    method_name = method[:params_start]\n",
    "    params_text = method[params_start:]\n",
    "    if len(params_text) > 2:\n",
    "        parameters_text = params_text[1:-1]  # remove ()\n",
    "        parameters = parameters_text.split(',')\n",
    "        params_types = []\n",
    "        for param in parameters:\n",
    "            param = param.strip(\" \")\n",
    "            if param.startswith(\"params \"):\n",
    "                param = param[len(\"params\"):]\n",
    "            elif param.startswith(\"out \"):\n",
    "                param = param[len(\"out\"):]\n",
    "            elif param.startswith(\"in \"):\n",
    "                param = param[len(\"in\"):]\n",
    "            elif param.startswith(\"ref \"):\n",
    "                param = param[len(\"ref\"):]\n",
    "            param = param.strip(\" \")\n",
    "            params_types.append(param)\n",
    "        params_text = '(' + ','.join(params_types) + ')'\n",
    "        \n",
    "    # for inner classes\n",
    "    method_name = method_name.replace('+', '.')\n",
    "\n",
    "    new_name = method_name + params_text\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def get_profiler_metrics_data(file):\n",
    "    calls_metrics = {'Method': [], 'Calls': []}\n",
    "    root = ElementTree.parse(file).getroot()\n",
    "    for type_tag in root.findall('Function'):\n",
    "        method = type_tag.get('FQN')\n",
    "        calls = type_tag.get('Calls')\n",
    "\n",
    "        calls_metrics['Method'].append(str(method))\n",
    "        calls_metrics['Calls'].append(int(calls))\n",
    "    df = pd.DataFrame(data=calls_metrics)\n",
    "    df['Method'] = df['Method'].apply(change_method_name_usage)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_profiler_metrics_data(folder):\n",
    "    calls_dfs = []\n",
    "    with os.scandir(folder) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file() and entry.name.endswith('.xml'):\n",
    "                full_path = os.path.join(folder, entry.name)\n",
    "                calls_dfs.append(get_profiler_metrics_data(full_path))\n",
    "    df = pd.concat(calls_dfs)\n",
    "    df = df.groupby('Method')['Calls'].sum().reset_index()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the profiler metrics and replace 'ctor' and 'cctor' with the class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calls_data = get_all_profiler_metrics_data(usage_folder)\n",
    "\n",
    "calls_data['Method'] = calls_data['Method'].apply(ctor_to_class_name)\n",
    "calls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra method to collect the metrics from Visual Studio Performance Profiler report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vs_profiler_metrics_data():\n",
    "    file = \"C:/Users/aprodea/work/profiler/vs/Report20201015-1152_FunctionSummary.xml\"\n",
    "    calls_metrics = {'Method': [], 'Calls': [], 'SourceFile': []}\n",
    "    root = ElementTree.parse(file).getroot()\n",
    "    for type_tag in root.findall('FunctionSummary/Function'):\n",
    "        method = type_tag.get('FunctionName')\n",
    "        calls = type_tag.get('NumCalls')\n",
    "        source_file = type_tag.get('SourceFile')\n",
    "        # line_no = type_tag.get('LineNumber')\n",
    "\n",
    "        if method.startswith('GES_GRT'):\n",
    "            calls_metrics['Method'].append(str(method))\n",
    "            calls_metrics['Calls'].append(int(calls))\n",
    "            calls_metrics['SourceFile'].append(str(source_file))\n",
    "\n",
    "    return pd.DataFrame(data=calls_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coverage = {'Method': [], 'TotalStatements': [], 'CoveredStatements': []}\n",
    "\n",
    "def get_children(all_path, element):\n",
    "    name = element.get(\"Name\")\n",
    "    if element.tag in ['Method', 'Constructor']:\n",
    "        all_path += name\n",
    "        method_name = str(all_path)\n",
    "        idx = method_name.rfind(':')\n",
    "        if idx > 0:\n",
    "            method_name = method_name[:idx]\n",
    "        test_coverage['Method'].append(method_name)\n",
    "        test_coverage['TotalStatements'].append(int(element.get(\"TotalStatements\")))\n",
    "        test_coverage['CoveredStatements'].append(str(element.get(\"CoveredStatements\")))\n",
    "    elif name is not None:\n",
    "        all_path += name + '.'\n",
    "        \n",
    "    for el in list(element):\n",
    "        get_children(all_path, el)\n",
    "\n",
    "\n",
    "def change_name_coverage(method):\n",
    "    start_parameters = method.rfind('(')\n",
    "    method_name = method[:start_parameters]\n",
    "    parameters_text = method[start_parameters:]\n",
    "    if len(parameters_text) > 2:\n",
    "        parameters_text = parameters_text.replace('params ', '')\n",
    "        parameters_text = parameters_text.replace('out ', '')\n",
    "        parameters_text = parameters_text.replace('ref ', '')\n",
    "        parameters_text = parameters_text.replace('in ', '')\n",
    "        parameters_text = replace_by_token(parameters_text, True, True)\n",
    "\n",
    "    new_name = method_name + parameters_text\n",
    "    return new_name\n",
    "        \n",
    "        \n",
    "def get_test_coverage_data(test_coverage_file):\n",
    "    root = ElementTree.parse(test_coverage_file).getroot()\n",
    "\n",
    "#     for type_tag in root.findall('Project/*'):\n",
    "    # in the new version we have assembly instead of project\n",
    "    for type_tag in root.findall('Assembly/*'):\n",
    "        get_children('', type_tag)\n",
    "\n",
    "    df = pd.DataFrame(data=test_coverage)\n",
    "    df['Method'] = df['Method'].apply(change_name_coverage)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test coverage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_coverage_data = None\n",
    "if test_coverage_file is not None:\n",
    "    test_coverage_data = get_test_coverage_data(test_coverage_file)\n",
    "test_coverage_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the resulting datasets to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sc_metrics_data.to_csv(save_to_folder + \"subset_metrics.csv\", sep=';', index=False)\n",
    "calls_data.to_csv(save_to_folder + \"all_calls.csv\", sep=';', index=False)\n",
    "change_data.to_csv(save_to_folder + \"all_changes.csv\", sep=';', index=False)\n",
    "\n",
    "if test_coverage_data is not None:\n",
    "    test_coverage_data.to_csv(save_to_folder + \"test_coverage.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff_ch_and_metrics = change_data[~change_data['Method_Parsed'].isin(sc_metrics_data['Method'])].dropna()\n",
    "diff_ch_and_metrics.to_csv(save_to_folder + \"ch_not_found_in_metrics.csv\", sep=';', index=False, na_rep=0)\n",
    "diff_ch_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_metrics_cov = None\n",
    "if test_coverage_data is not None:\n",
    "    diff_metrics_cov = sc_metrics_data[~sc_metrics_data['Method'].isin(test_coverage_data['Method'])].dropna()\n",
    "    for m in diff_metrics_cov['Method']:\n",
    "        print(m)\n",
    "diff_metrics_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the change metrics we need and rename the columns, and sum the results (in case there are multiple rows for the same method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_change_data = change_data[['Method_Parsed', 'Changes']]\n",
    "sub_change_data.columns = ['Method', 'NChg']\n",
    "sub_change_data = sub_change_data.groupby('Method')['NChg'].sum().reset_index()\n",
    "sub_change_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge change data to soure code metrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_left_ch = pd.merge(left=sc_metrics_data, right=sub_change_data, how='left', left_on='Method', right_on='Method')\n",
    "merged_left_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "missing_ch = merged_left_ch[pd.isnull(merged_left_ch['NChg'])]\n",
    "missing_ch.to_csv(save_to_folder + \"missing_ch_in_merged.csv\", sep=';', index=False)\n",
    "missing_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in missing_ch['Method']:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the merging result; remove the generics type of methods in order to match with the method names from profiler report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_generics_as_in_calls(params_decl):\n",
    "    new_decl = ''\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(params_decl)):\n",
    "        c = params_decl[i]\n",
    "        if c == '<':\n",
    "            count += 1\n",
    "            continue\n",
    "        if c == '>':\n",
    "            count -= 1\n",
    "            continue\n",
    "        if c == '&':\n",
    "            continue\n",
    "        if count == 0:\n",
    "            new_decl += c\n",
    "\n",
    "    return new_decl\n",
    "\n",
    "\n",
    "merged_left_ch['Method_Parsed'] = merged_left_ch['Method'].apply(remove_generics_as_in_calls)\n",
    "merged_left_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the profiler metrics we need and rename the columns, and sum the results (in case there are multiple rows for the same method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_calls_data = calls_data[['Method', 'Calls']]\n",
    "sub_calls_data.columns = ['Method_calls', 'NCall']\n",
    "sub_calls_data = sub_calls_data.groupby('Method_calls')['NCall'].sum().reset_index()\n",
    "sub_calls_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge also the profiler metrics based on the renamed variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_left_ca = pd.merge(left=merged_left_ch, right=sub_calls_data, how='left', left_on='Method_Parsed', right_on='Method_calls')\n",
    "missing_merged_left_ca = merged_left_ca[pd.isnull(merged_left_ca['NCall'])]\n",
    "merged_left_ca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the renamed variable and the variable from the profiler, to keep only the previous method name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_left_ca.drop(['Method_Parsed', 'Method_calls'], axis=1, inplace=True)\n",
    "merged_left_ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_left_ca.to_csv(save_to_folder + \"merged.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how many rows do not have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing rows with 1 for change (assuming that every method had at least the initial commit), and 0 for the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca['NChg'].fillna(1, inplace=True)\n",
    "merged_left_ca['NCall'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the resulting dataset with filled missing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca.to_csv(save_to_folder + \"merged_filledna.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge metrics with test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "metrics_file = save_to_folder + \"merged_filledna.csv\"\n",
    "metrics_data = pd.read_csv(metrics_file, sep=';')\n",
    "\n",
    "test_cov_file = save_to_folder + \"test_coverage.csv\"\n",
    "data_combined = None\n",
    "if Path(test_cov_file).is_file():\n",
    "    test_data = pd.read_csv(test_cov_file, sep=';')\n",
    "\n",
    "    data_combined = pd.merge(metrics_data, test_data, on='Method', how='left')\n",
    "\n",
    "    data_combined.to_csv(save_to_folder + \"merged_complete.csv\", sep=';', index=False)\n",
    "\n",
    "data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined is not None:\n",
    "    print(data_combined.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = merged_left_ca.columns.tolist()\n",
    "list_columns.remove('Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "merged_left_ca[list_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_left_ca['NCall'].describe()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left_ca['LOC'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,15))\n",
    "ax= axes.flatten()\n",
    "for i in range(len(list_columns)):\n",
    "    # ax = plt.subplot(10, 1, i+1)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(16)\n",
    "    col_name = list_columns[i]\n",
    "    sm.qqplot(merged_left_ca[col_name], marker='o', markerfacecolor='none', markeredgecolor='k', alpha=0.5,\n",
    "              ax = ax[i])\n",
    "    ax[i].set_ylabel(col_name)\n",
    "\n",
    "# plt.tight_layout(pad=1.5)\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(save_to_folder + 'plots/qqplots_unscaled.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats.probplot(change_data['Changes'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(merged_left_ca['NP'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustertend import hopkins, vat, assess_tendency_by_mean_metric_score\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, minmax_scale, RobustScaler,robust_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = robust_scale(merged_left_ca[list_columns])\n",
    "hopkins(X, merged_left_ca.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = minmax_scale(merged_left_ca[list_columns])\n",
    "hopkins(X, merged_left_ca.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = merged_left_ca.copy()\n",
    "\n",
    "for col_name in list_columns:\n",
    "    col = scaled_data[col_name]\n",
    "    min_col, max_col = col.min(), col.max()\n",
    "#     print(col_name, min_col, max_col)\n",
    "    scaled_data[col_name] = (col - min_col) / (max_col - min_col)\n",
    "    \n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopkins(scaled_data[list_columns], scaled_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,15))\n",
    "ax= axes.flatten()\n",
    "for i in range(len(list_columns)):\n",
    "    col_name = list_columns[i]\n",
    "    sm.qqplot(scaled_data[col_name], marker='o', markerfacecolor='none', markeredgecolor='k', alpha=0.5,\n",
    "              ax = ax[i])\n",
    "    ax[i].set_ylabel(col_name)\n",
    "\n",
    "# plt.tight_layout(pad=1.5)\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(save_to_folder + '/plots/qqplots_scaled.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "x = merged_left_ca['CC']\n",
    "x = x[~np.isnan(x)]\n",
    "print(Counter(x))\n",
    "\n",
    "# plt.hist(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
