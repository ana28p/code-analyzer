{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from xml.etree import ElementTree\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from pyclustertend import hopkins, vat, assess_tendency_by_mean_metric_score\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, minmax_scale, RobustScaler,robust_scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "import sklearn.metrics as compute_metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels_file = None\n",
    "\n",
    "metrics_labelled_file = \"/classification_all/complete_classification.csv\"\n",
    "\n",
    "chg_lines_file = \"/changed_lines.csv\"\n",
    "\n",
    "var_list = ['LOC', 'CC', 'NP', 'NV', 'NEST', 'Ca', 'Ce', 'NChg', 'NCall']\n",
    "label_list = ['CLevel_threshold', 'CLevel_k_means', 'CLevel_em']\n",
    "\n",
    "def get_labelled_data():\n",
    "    data = pd.read_csv(metrics_labelled_file, sep=';')\n",
    "    return data\n",
    "\n",
    "labelled_df = get_labelled_data()\n",
    "labelled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using expert knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels_data = None\n",
    "if real_labels_file is not None:\n",
    "    real_labels_data = pd.read_csv(real_labels_file, sep=';')\n",
    "    print(real_labels_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_cm(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    column_width = 10\n",
    "    # Print header\n",
    "    header = \" \" * column_width\n",
    "    for label in labels:\n",
    "        header += \"%{0}s\".format(column_width) % label\n",
    "    print(header)\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        row_text = \"%{0}s\".format(column_width) % label1\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.1f\".format(column_width) % cm[i, j]\n",
    "            row_text += cell\n",
    "        print(row_text)\n",
    "\n",
    "\n",
    "def classification_report(real, predicted):\n",
    "    labels = ['high', 'regular', 'low']\n",
    "    ari = adjusted_rand_score(labels_true=real, labels_pred=predicted)\n",
    "    acc = compute_metrics.accuracy_score(y_true=real, y_pred=predicted)\n",
    "    report = compute_metrics.classification_report(y_true=real, y_pred=predicted, labels=labels)\n",
    "    conf_matrix = compute_metrics.confusion_matrix(y_true=real, y_pred=predicted, labels=labels)\n",
    "    print('ARI ', ari)\n",
    "    print('Accuracy ', acc)\n",
    "    print(report)\n",
    "    print('Confusion matrix')\n",
    "    print_cm(conf_matrix, labels)\n",
    "\n",
    "\n",
    "def classification_report_for_all():\n",
    "    real_labels_data = pd.read_csv(real_labels_file, sep=';')\n",
    "    \n",
    "    metrics_labelled_data = pd.read_csv(metrics_labelled_file, sep=';')\n",
    "\n",
    "    data_combined = pd.merge(left=real_labels_data[['Method', 'CLevel']],\n",
    "                             right=metrics_labelled_data[['Method', 'CLevel_threshold', 'CLevel_k_means', 'CLevel_em']],\n",
    "                             on='Method', how='inner')\n",
    "\n",
    "    pred_labels_var = ['CLevel_threshold', 'CLevel_k_means', 'CLevel_em']\n",
    "    for y_pred in pred_labels_var:\n",
    "        print('------- {} ------'.format(y_pred))\n",
    "        classification_report(data_combined['CLevel'], data_combined[y_pred])\n",
    "\n",
    "if real_labels_data is not None:\n",
    "    classification_report_for_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using changed lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chg_lines_data = pd.read_csv(chg_lines_file, sep=';')\n",
    "chg_lines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(labelled_df, chg_lines_data[['Previous_Method_Parsed', 'ChgLines']], how='inner', left_on='Method', right_on='Previous_Method_Parsed')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ChgLines']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dict = {'low': 0, 'regular': 1, 'high': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df1 = df[['Method','CLevel_threshold', 'ChgLines']]\n",
    "sub_df1 = sub_df1.sort_values(by=['CLevel_threshold'], key=lambda x: x.map(custom_dict), ignore_index=True)\n",
    "sub_df1['method_idx'] = sub_df1.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,4), dpi= 80)\n",
    "sns.scatterplot(data=sub_df1, x=\"method_idx\", y=\"ChgLines\", hue=\"CLevel_threshold\", linewidth=0, alpha=0.5,\n",
    "                palette={'low':'blue', 'regular':'#DCB732', 'high':'red'})\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Number of changed lines after the threshold-based clustering')\n",
    "plt.ylabel('Number of lines changed')\n",
    "plt.xlabel('Methods')\n",
    "# plt.savefig(plots_save_to_location + '/chg-lines_threshold.pdf', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df1 = sub_df1.groupby('CLevel_threshold')\n",
    "print(grouped_df1[['ChgLines']].sum())\n",
    "grouped_df1[['ChgLines']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df2 = df[['Method', 'CLevel_k_means', 'ChgLines']]\n",
    "sub_df2 = sub_df2.sort_values(by=['CLevel_k_means'], key=lambda x: x.map(custom_dict), ignore_index=True)\n",
    "sub_df2['method_idx'] = sub_df2.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,4), dpi= 80)\n",
    "sns.scatterplot(data=sub_df2, x=\"method_idx\", y=\"ChgLines\", hue=\"CLevel_k_means\", \n",
    "                palette={'low':'blue', 'regular':'#DCB732', 'high':'red'})\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Number of changed lines after the K-means clustering')\n",
    "plt.xlabel('Methods')\n",
    "plt.savefig(plots_save_to_location + '/chg-lines_k_means.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df2 = sub_df2.groupby('CLevel_k_means')\n",
    "print(grouped_df2[['ChgLines']].sum())\n",
    "grouped_df2[['ChgLines']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df3 = df[['Method', 'CLevel_em', 'ChgLines']]\n",
    "sub_df3 = sub_df3.sort_values(by=['CLevel_em'], key=lambda x: x.map(custom_dict), ignore_index=True)\n",
    "sub_df3['method_idx'] = sub_df3.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,4), dpi= 80)\n",
    "sns.scatterplot(data=sub_df3, x=\"method_idx\", y=\"ChgLines\", hue=\"CLevel_em\", \n",
    "                palette={'low':'blue', 'regular':'#DCB732', 'high':'red'})\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Number of changed lines after the EM clustering')\n",
    "plt.xlabel('Methods')\n",
    "plt.savefig(plots_save_to_location + '/chg-lines_em.pdf', bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df3 = sub_df3.groupby('CLevel_em')\n",
    "print(grouped_df3[['ChgLines']].sum())\n",
    "grouped_df3[['ChgLines']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df[['Method','CLevel_threshold','CLevel_k_means','CLevel_em', 'ChgLines']]\n",
    "m_df = pd.melt(df, id_vars=['Method', 'ChgLines'], value_vars=['CLevel_threshold','CLevel_k_means','CLevel_em'])\n",
    "m_df.columns = ['Method', 'ChgLines', 'CType', 'CLevel']\n",
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi= 80)    \n",
    "sns.stripplot(data=m_df, x='CType', y='ChgLines', hue='CLevel',\n",
    "              palette={'low':'blue', 'regular':'#DCB732', 'high':'red'},\n",
    "              hue_order=[\"low\", \"regular\", \"high\"],\n",
    "              jitter=0.25, size=5, ax=ax, linewidth=.3, dodge=True)\n",
    "# plt.title('Results for the threshold approach')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed lines correlation with the other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = df.copy()\n",
    "list_columns = ['LOC', 'CC', 'NP', 'NV', 'NEST', 'Ca', 'Ce', 'NChg', 'NCall', 'ChgLines']\n",
    "\n",
    "for col_name in list_columns:\n",
    "    col = scaled_data[col_name]\n",
    "    min_col, max_col = col.min(), col.max()\n",
    "#     min_col = 0  # consider min as 0 to perserve the importance of values; eg LOC 25, 50 -> 0.5, 1 \n",
    "#     print(col_name, min_col, max_col)\n",
    "    scaled_data[col_name] = (col - min_col) / (max_col - min_col)\n",
    "    \n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corr = scaled_data[list_columns].corr(method='kendall')\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(p_corr, xticklabels=p_corr.columns, yticklabels=p_corr.columns, annot=True, cmap='coolwarm', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corr = df[list_columns].corr(method='kendall')\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(p_corr, xticklabels=p_corr.columns, yticklabels=p_corr.columns, annot=True, cmap='coolwarm', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
